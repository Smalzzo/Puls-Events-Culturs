name: Tests et Évaluation Continue

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Exécution quotidienne à 6h UTC pour l'évaluation RAGAS
    - cron: '0 6 * * *'

jobs:
  tests-unitaires:
    name: Tests Unitaires
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ['3.11']
    
    steps:
    - name: Checkout du code
      uses: actions/checkout@v4
    
    - name: Configuration Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}
        cache: 'pip'
    
    - name: Installation des dépendances
      run: |
        python -m pip install --upgrade pip
        pip install -e .
        pip install pytest pytest-cov pytest-mock pytest-asyncio
    
    - name: Exécution des tests unitaires
      run: |
        pytest -m "unit" --cov=src --cov-report=xml --cov-report=term -v
    
    - name: Upload de la couverture vers Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella

  tests-integration:
    name: Tests d'Intégration
    runs-on: ubuntu-latest
    needs: tests-unitaires
    
    steps:
    - name: Checkout du code
      uses: actions/checkout@v4
    
    - name: Configuration Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'
    
    - name: Installation des dépendances
      run: |
        python -m pip install --upgrade pip
        pip install -e .
        pip install pytest pytest-mock
    
    - name: Exécution des tests d'intégration
      run: |
        pytest -m "integration" -v

  tests-performance:
    name: Tests de Performance
    runs-on: ubuntu-latest
    needs: tests-unitaires
    if: github.event_name == 'push' || github.event_name == 'schedule'
    
    steps:
    - name: Checkout du code
      uses: actions/checkout@v4
    
    - name: Configuration Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'
    
    - name: Installation des dépendances
      run: |
        python -m pip install --upgrade pip
        pip install -e .
        pip install pytest pytest-mock numpy
    
    - name: Exécution des tests de performance
      run: |
        pytest tests/test_performance.py -v --durations=10
    
    - name: Upload des résultats de performance
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: performance-results
        path: performance_report.json
        retention-days: 30

  evaluation-ragas:
    name: Évaluation RAGAS Automatisée
    runs-on: ubuntu-latest
    needs: tests-unitaires
    # Exécuter uniquement sur schedule ou manuellement
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
    
    steps:
    - name: Checkout du code
      uses: actions/checkout@v4
    
    - name: Configuration Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'
    
    - name: Installation des dépendances
      run: |
        python -m pip install --upgrade pip
        pip install -e .
        pip install ragas datasets
    
    - name: Création du répertoire de données de test
      run: |
        mkdir -p data/test
        # Créer un fichier de test minimal si nécessaire
        echo '[{"question": "Test question?", "ground_truth": "Test answer"}]' > data/test/ragas_questions_mini.json
    
    - name: Exécution de l'évaluation RAGAS
      env:
        MISTRAL_API_KEY: ${{ secrets.MISTRAL_API_KEY }}
      run: |
        python scripts/run_automated_evaluation.py \
          --test-file data/test/ragas_questions_mini.json \
          --output-dir data/evaluations \
          --trend-report \
          --export-csv
      continue-on-error: true
    
    - name: Upload des rapports d'évaluation
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: ragas-evaluation-reports
        path: |
          data/evaluations/*.json
          data/evaluations/*.csv
        retention-days: 90
    
    - name: Vérification des seuils de qualité
      run: |
        python -c "
        import json
        from pathlib import Path
        
        # Charger le dernier rapport
        reports = sorted(Path('data/evaluations').glob('evaluation_*.json'))
        if reports:
            with open(reports[-1]) as f:
                report = json.load(f)
            
            status = report.get('status', 'unknown')
            print(f'Status: {status}')
            
            if status == 'critical':
                print('❌ Évaluation critique - Intervention requise')
                exit(1)
            elif status == 'warning':
                print('⚠️  Évaluation avec alertes')
                exit(0)
            else:
                print('✅ Évaluation réussie')
                exit(0)
        "

  lint:
    name: Linting et Formatage
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout du code
      uses: actions/checkout@v4
    
    - name: Configuration Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'
    
    - name: Linting désactivé
      run: |
        echo "Linting désactivé: Ruff supprimé du pipeline."

  rapport-qualite:
    name: Rapport de Qualité Global
    runs-on: ubuntu-latest
    needs: [tests-unitaires, tests-integration, lint]
    if: always()
    
    steps:
    - name: Génération du badge de qualité
      run: |
        echo "✅ Pipeline de qualité complété" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## Résumé des tests" >> $GITHUB_STEP_SUMMARY
        echo "- Tests unitaires: ${{ needs.tests-unitaires.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- Tests d'intégration: ${{ needs.tests-integration.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- Linting: ${{ needs.lint.result }}" >> $GITHUB_STEP_SUMMARY
