# Guide des Tests - Puls Events Culturs RAG

Ce document explique comment ex√©cuter et utiliser les diff√©rents types de tests du projet.

## üìã Table des mati√®res

- [Types de tests](#types-de-tests)
- [Installation](#installation)
- [Ex√©cution des tests](#ex√©cution-des-tests)
- [Tests de performance](#tests-de-performance)
- [√âvaluation automatis√©e](#√©valuation-automatis√©e)
- [Couverture de code](#couverture-de-code)
- [CI/CD](#cicd)

## üß™ Types de tests

### Tests unitaires (`test_*.py`)

Tests rapides qui valident les composants individuels :

- **`test_config.py`** : Configuration et param√®tres
- **`test_indexer.py`** : Tests de base de l'indexeur
- **`test_indexer_advanced.py`** : Tests avanc√©s d'indexation (qualit√©, robustesse)
- **`test_api.py`** : Tests des endpoints API

### Tests de performance (`test_performance.py`)

Tests qui mesurent les performances du syst√®me :

- Temps de r√©ponse des requ√™tes
- Charge et acc√®s concurrent
- Scalabilit√© avec grands volumes
- Utilisation des ressources

### Tests d'√©valuation (`test_ragas_automation.py`)

Tests pour l'automatisation des m√©triques RAGAS :

- Configuration de l'√©valuateur
- Cr√©ation de datasets d'√©valuation
- Validation des m√©triques
- D√©tection de r√©gressions

## üîß Installation

Installer les d√©pendances de test :

```bash
# Windows PowerShell
.\.venv\Scripts\python.exe -m pip install pytest pytest-cov pytest-mock pytest-asyncio

# Linux/Mac
.venv/bin/python -m pip install pytest pytest-cov pytest-mock pytest-asyncio
```

## ‚ñ∂Ô∏è Ex√©cution des tests

### Tous les tests

```bash
# Windows
.\.venv\Scripts\pytest.exe

# Linux/Mac
.venv/bin/pytest
```

### Tests par marqueur

```bash
# Tests unitaires uniquement (rapides)
.\.venv\Scripts\pytest.exe -m unit

# Tests d'int√©gration
.\.venv\Scripts\pytest.exe -m integration

# Tests de performance
.\.venv\Scripts\pytest.exe -m performance

# Tests d'√©valuation RAGAS (lents)
.\.venv\Scripts\pytest.exe -m evaluation
```

### Tests par fichier

```bash
# Tests d'indexation avanc√©s
.\.venv\Scripts\pytest.exe tests/test_indexer_advanced.py

# Tests de performance
.\.venv\Scripts\pytest.exe tests/test_performance.py

# Tests d'√©valuation
.\.venv\Scripts\pytest.exe tests/test_ragas_automation.py
```

### Tests par classe ou fonction

```bash
# Une classe sp√©cifique
.\.venv\Scripts\pytest.exe tests/test_indexer_advanced.py::TestIndexDataQuality

# Une fonction sp√©cifique
.\.venv\Scripts\pytest.exe tests/test_performance.py::TestRAGPerformance::test_query_response_time_single
```

### Options utiles

```bash
# Mode verbeux avec sortie d√©taill√©e
.\.venv\Scripts\pytest.exe -v

# Arr√™ter √† la premi√®re erreur
.\.venv\Scripts\pytest.exe -x

# Afficher les print statements
.\.venv\Scripts\pytest.exe -s

# Ex√©cuter les N tests les plus lents
.\.venv\Scripts\pytest.exe --durations=10

# Ex√©cuter en parall√®le (n√©cessite pytest-xdist)
.\.venv\Scripts\pytest.exe -n auto
```

## üìä Tests de performance

Les tests de performance mesurent :

1. **Temps de r√©ponse** : Latence des requ√™tes individuelles
2. **D√©bit** : Nombre de requ√™tes par seconde
3. **Charge concurrente** : Comportement sous charge parall√®le
4. **Scalabilit√©** : Performance avec grands volumes de donn√©es

### Ex√©cution

```bash
# Tous les tests de performance
.\.venv\Scripts\pytest.exe tests/test_performance.py -v

# Avec rapport de temps d'ex√©cution
.\.venv\Scripts\pytest.exe tests/test_performance.py --durations=0
```

### M√©triques collect√©es

- Temps de r√©ponse moyen, m√©dian, P95, P99
- Throughput (requ√™tes/seconde)
- Temps de construction d'index
- Utilisation m√©moire

## ü§ñ √âvaluation automatis√©e

### Script d'√©valuation automatique

Le script `scripts/run_automated_evaluation.py` permet d'automatiser les √©valuations RAGAS :

```bash
# √âvaluation avec le fichier mini (rapide)
.\.venv\Scripts\python.exe scripts/run_automated_evaluation.py

# √âvaluation avec le fichier complet
.\.venv\Scripts\python.exe scripts/run_automated_evaluation.py --test-file data/test/ragas_questions.json

# Avec Mistral embeddings
.\.venv\Scripts\python.exe scripts/run_automated_evaluation.py --use-mistral-embeddings

# G√©n√©rer un rapport de tendance
.\.venv\Scripts\python.exe scripts/run_automated_evaluation.py --trend-report

# Exporter l'historique en CSV
.\.venv\Scripts\python.exe scripts/run_automated_evaluation.py --export-csv
```

### Fonctionnalit√©s

- ‚úÖ Ex√©cution automatique des √©valuations RAGAS
- ‚úÖ Historique des √©valuations avec timestamps
- ‚úÖ D√©tection d'alertes (scores sous seuils)
- ‚úÖ D√©tection de r√©gressions (baisse de performance)
- ‚úÖ G√©n√©ration de rapports JSON
- ‚úÖ Export CSV pour analyse
- ‚úÖ Rapports de tendance

### Seuils de qualit√©

Les seuils par d√©faut sont :

```python
{
    "faithfulness": 0.70,          # Fid√©lit√© aux sources
    "answer_relevancy": 0.70,      # Pertinence de la r√©ponse
    "context_precision": 0.65,     # Pr√©cision du contexte
    "context_recall": 0.65         # Rappel du contexte
}
```

### Interpr√©tation des r√©sultats

- **Success** : Tous les scores au-dessus des seuils, aucune r√©gression
- **Warning** : Alertes ou r√©gressions de s√©v√©rit√© moyenne
- **Critical** : Alertes ou r√©gressions de haute s√©v√©rit√©

### Rapports g√©n√©r√©s

Les rapports sont sauvegard√©s dans `data/evaluations/` :

- `evaluation_YYYYMMDD_HHMMSS.json` : Rapport d'√©valuation individuel
- `evaluation_history.json` : Historique complet
- `trend_report.json` : Rapport de tendance
- `metrics_history.csv` : Export CSV pour Excel/Google Sheets

## üìà Couverture de code

### Ex√©cuter avec couverture

```bash
# G√©n√©rer la couverture
.\.venv\Scripts\pytest.exe --cov=src --cov-report=html --cov-report=term

# Voir le rapport HTML
start htmlcov/index.html  # Windows
open htmlcov/index.html   # Mac
xdg-open htmlcov/index.html  # Linux
```

### Rapport de couverture

Le rapport HTML d√©taille :

- Pourcentage de code couvert par fichier
- Lignes non couvertes (en rouge)
- Branches non test√©es
- Statistiques globales

### Objectifs de couverture

- **Minimum acceptable** : 70%
- **Objectif** : 80%
- **Excellent** : 90%+

## üîÑ CI/CD

### Tests pour int√©gration continue

```bash
# Tests rapides pour CI (exclure les tests lents)
.\.venv\Scripts\pytest.exe -m "not slow" --tb=short

# Tests avec code de sortie
.\.venv\Scripts\pytest.exe -x --tb=short || exit 1

# G√©n√©rer un rapport XML pour CI
.\.venv\Scripts\pytest.exe --junitxml=test-results.xml
```

### Configuration GitHub Actions (exemple)

```yaml
name: Tests

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - uses: actions/setup-python@v2
        with:
          python-version: '3.11'
      - name: Install dependencies
        run: |
          pip install -e .
          pip install pytest pytest-cov
      - name: Run tests
        run: pytest -m "not slow" --cov=src --cov-report=xml
      - name: Upload coverage
        uses: codecov/codecov-action@v2
```

## üõ†Ô∏è Commandes utiles

### Lancer les tests depuis VS Code

Utiliser les t√¢ches configur√©es :

```bash
# T√¢che "Test: Run All Tests"
Ctrl+Shift+P > Tasks: Run Task > Test: Run All Tests
```

### Debugging de tests

```python
# Ajouter un breakpoint dans pytest
import pytest
pytest.set_trace()

# Ou utiliser le debugger VS Code
# Mettre un breakpoint et lancer en mode debug
```

### G√©n√©rer un nouveau fichier de test

```bash
# Cr√©er √† partir du template
cp tests/test_template.py tests/test_new_feature.py
```

## üìù Bonnes pratiques

1. **√âcrire des tests avant le code** (TDD quand possible)
2. **Nommer les tests de mani√®re descriptive** : `test_should_do_something_when_condition`
3. **Un test = une assertion** (dans l'id√©al)
4. **Utiliser les fixtures** pour r√©duire la duplication
5. **Marquer les tests lents** avec `@pytest.mark.slow`
6. **Mocker les appels externes** (API, base de donn√©es)
7. **Tester les cas limites** (valeurs nulles, vides, extr√™mes)
8. **Documenter les tests complexes** avec des docstrings

## üêõ R√©solution de probl√®mes

### Les tests ne s'ex√©cutent pas

```bash
# V√©rifier l'installation de pytest
.\.venv\Scripts\python.exe -m pytest --version

# R√©installer si n√©cessaire
.\.venv\Scripts\python.exe -m pip install --upgrade pytest
```

### Erreurs d'import

```bash
# V√©rifier le PYTHONPATH
.\.venv\Scripts\python.exe -c "import sys; print('\n'.join(sys.path))"

# Installer le projet en mode √©ditable
.\.venv\Scripts\python.exe -m pip install -e .
```

### Tests lents

```bash
# Identifier les tests lents
.\.venv\Scripts\pytest.exe --durations=20

# Exclure les tests lents
.\.venv\Scripts\pytest.exe -m "not slow"
```

## üìö Ressources

- [Pytest Documentation](https://docs.pytest.org/)
- [Pytest Coverage](https://pytest-cov.readthedocs.io/)
- [RAGAS Documentation](https://docs.ragas.io/)
- [Testing Best Practices](https://testdriven.io/blog/testing-best-practices/)

## ü§ù Contribution

Pour ajouter de nouveaux tests :

1. Cr√©er un fichier `test_*.py` dans `tests/`
2. Utiliser les fixtures de `conftest.py`
3. Ajouter les marqueurs appropri√©s
4. Documenter le test avec une docstring
5. Ex√©cuter les tests pour v√©rifier

---

**Note** : Ce guide est maintenu et mis √† jour r√©guli√®rement. Pour toute question, consulter l'√©quipe de d√©veloppement.
