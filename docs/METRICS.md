# M√©triques d'√âvaluation RAG - Guide Complet

Ce document explique les m√©triques utilis√©es pour √©valuer la qualit√© du syst√®me RAG et comment les interpr√©ter.

## üìä M√©triques RAGAS

### 1. Faithfulness (Fid√©lit√©)

**D√©finition** : Mesure si la r√©ponse g√©n√©r√©e est fid√®le aux sources/contextes fournis, sans hallucination.

**Calcul** : 
```
Faithfulness = Nombre de d√©clarations soutenues par les sources / Nombre total de d√©clarations
```

**Interpr√©tation** :
- **0.90 - 1.00** : Excellent - Tr√®s peu ou pas d'hallucinations
- **0.70 - 0.89** : Bon - Quelques impr√©cisions mineures
- **0.50 - 0.69** : Moyen - Hallucinations notables
- **< 0.50** : Mauvais - Hallucinations fr√©quentes

**Importance** : ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Critique)

**Actions si score faible** :
- V√©rifier la qualit√© des chunks/contextes
- Am√©liorer le prompt pour r√©duire les hallucinations
- Augmenter le nombre de sources r√©cup√©r√©es
- Revoir le syst√®me de reranking

### 2. Answer Relevancy (Pertinence de la R√©ponse)

**D√©finition** : Mesure si la r√©ponse est pertinente par rapport √† la question pos√©e.

**Calcul** : Bas√© sur la similarit√© s√©mantique entre la question et la r√©ponse g√©n√©r√©e.

**Interpr√©tation** :
- **0.90 - 1.00** : Excellent - R√©ponse tr√®s pertinente
- **0.70 - 0.89** : Bon - R√©ponse g√©n√©ralement pertinente
- **0.50 - 0.69** : Moyen - R√©ponse partiellement pertinente
- **< 0.50** : Mauvais - R√©ponse hors sujet

**Importance** : ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Critique)

**Actions si score faible** :
- Am√©liorer le prompt syst√®me
- Optimiser la r√©cup√©ration de contexte
- V√©rifier la qualit√© des embeddings
- Ajuster les param√®tres de temp√©rature du LLM

### 3. Context Precision (Pr√©cision du Contexte)

**D√©finition** : Mesure la proportion de contextes r√©cup√©r√©s qui sont r√©ellement pertinents pour la question.

**Calcul** :
```
Context Precision = Contextes pertinents r√©cup√©r√©s / Total contextes r√©cup√©r√©s
```

**Interpr√©tation** :
- **0.80 - 1.00** : Excellent - Peu de bruit dans les contextes
- **0.60 - 0.79** : Bon - Quelques contextes non pertinents
- **0.40 - 0.59** : Moyen - Beaucoup de bruit
- **< 0.40** : Mauvais - Majorit√© de contextes non pertinents

**Importance** : ‚≠ê‚≠ê‚≠ê‚≠ê

**Actions si score faible** :
- Am√©liorer le chunking (taille, overlap)
- Optimiser les embeddings
- Impl√©menter/am√©liorer le reranking
- Ajuster le nombre k de r√©sultats r√©cup√©r√©s

### 4. Context Recall (Rappel du Contexte)

**D√©finition** : Mesure la proportion d'informations n√©cessaires qui ont √©t√© r√©cup√©r√©es.

**Calcul** :
```
Context Recall = Informations n√©cessaires r√©cup√©r√©es / Total informations n√©cessaires
```

**Interpr√©tation** :
- **0.80 - 1.00** : Excellent - Toutes les infos n√©cessaires r√©cup√©r√©es
- **0.60 - 0.79** : Bon - Quelques infos manquantes
- **0.40 - 0.59** : Moyen - Beaucoup d'infos manquantes
- **< 0.40** : Mauvais - Informations critiques manquantes

**Importance** : ‚≠ê‚≠ê‚≠ê‚≠ê

**Actions si score faible** :
- Augmenter k (nombre de r√©sultats)
- Am√©liorer la qualit√© de l'indexation
- Enrichir les m√©tadonn√©es
- V√©rifier la compl√©tude des donn√©es sources

## üéØ Seuils Recommand√©s

### Environnement de Production

```python
PRODUCTION_THRESHOLDS = {
    "faithfulness": 0.85,          # Critique
    "answer_relevancy": 0.85,      # Critique
    "context_precision": 0.75,     # Important
    "context_recall": 0.75         # Important
}
```

### Environnement de D√©veloppement

```python
DEV_THRESHOLDS = {
    "faithfulness": 0.70,
    "answer_relevancy": 0.70,
    "context_precision": 0.65,
    "context_recall": 0.65
}
```

### Environnement de Test

```python
TEST_THRESHOLDS = {
    "faithfulness": 0.60,
    "answer_relevancy": 0.60,
    "context_precision": 0.55,
    "context_recall": 0.55
}
```

## üìà Score de Qualit√© Global

### Calcul Simple (Moyenne)

```python
quality_score = (faithfulness + answer_relevancy + context_precision + context_recall) / 4
```

### Calcul Pond√©r√© (Recommand√©)

```python
weights = {
    "faithfulness": 0.35,          # Le plus important
    "answer_relevancy": 0.35,      # Le plus important
    "context_precision": 0.15,
    "context_recall": 0.15
}

quality_score = sum(metric * weights[name] for name, metric in metrics.items())
```

### Interpr√©tation du Score Global

- **0.85 - 1.00** : üü¢ Excellent - Syst√®me pr√™t pour production
- **0.70 - 0.84** : üü° Bon - Am√©liorations mineures recommand√©es
- **0.55 - 0.69** : üü† Moyen - Am√©liorations importantes n√©cessaires
- **< 0.55** : üî¥ Mauvais - R√©vision compl√®te requise

## üîç Analyse des R√©gressions

### D√©tection de R√©gression

Une r√©gression est d√©tect√©e quand :

```python
change = current_score - previous_score
regression_threshold = -0.05  # -5%

if change < regression_threshold:
    # R√©gression d√©tect√©e
    severity = "high" if change < -0.10 else "medium"
```

### S√©v√©rit√© des R√©gressions

- **Haute** : Baisse > 10% ‚Üí Intervention imm√©diate
- **Moyenne** : Baisse 5-10% ‚Üí Investigation requise
- **Faible** : Baisse < 5% ‚Üí Monitoring

## üìä M√©triques de Performance

### Temps de R√©ponse

- **Excellent** : < 1s
- **Bon** : 1-3s
- **Acceptable** : 3-5s
- **Lent** : > 5s

### D√©bit (Throughput)

- **Excellent** : > 10 req/s
- **Bon** : 5-10 req/s
- **Acceptable** : 1-5 req/s
- **Faible** : < 1 req/s

### Temps de Construction d'Index

Pour 1000 √©v√©nements :
- **Excellent** : < 2 min
- **Bon** : 2-5 min
- **Acceptable** : 5-10 min
- **Lent** : > 10 min

## üìã Checklist d'√âvaluation

### Avant D√©ploiement

- [ ] Toutes les m√©triques RAGAS > seuils de production
- [ ] Aucune r√©gression d√©tect√©e
- [ ] Temps de r√©ponse < 3s (P95)
- [ ] Tests de charge r√©ussis
- [ ] Couverture de code > 80%
- [ ] Tous les tests unitaires passent

### Monitoring Continu

- [ ] √âvaluation RAGAS hebdomadaire
- [ ] Suivi des tendances
- [ ] Alertes configur√©es
- [ ] Historique des √©valuations sauvegard√©
- [ ] Rapports g√©n√©r√©s automatiquement

## üö® Alertes et Actions

### Alerte Critique

**Conditions** :
- Faithfulness < 0.70
- Answer Relevancy < 0.70
- R√©gression > 10%

**Actions** :
1. Arr√™ter les d√©ploiements
2. Analyser les logs
3. Identifier la cause
4. Corriger le probl√®me
5. Re-tester
6. Red√©ployer

### Alerte Warning

**Conditions** :
- M√©triques entre seuil - 0.10 et seuil
- R√©gression 5-10%

**Actions** :
1. Investiguer la cause
2. Planifier des am√©liorations
3. Augmenter la fr√©quence de monitoring
4. Documenter les observations

## üìù Exemple d'Interpr√©tation

### Sc√©nario 1 : R√©sultats Excellents

```json
{
  "faithfulness": 0.92,
  "answer_relevancy": 0.89,
  "context_precision": 0.85,
  "context_recall": 0.83
}
```

**Interpr√©tation** :
- ‚úÖ Syst√®me tr√®s fiable (faithfulness √©lev√©)
- ‚úÖ R√©ponses pertinentes
- ‚úÖ Contextes bien cibl√©s
- ‚úÖ Bonne couverture des informations
- **Action** : Continuer le monitoring, pr√™t pour production

### Sc√©nario 2 : Probl√®me de Contexte

```json
{
  "faithfulness": 0.88,
  "answer_relevancy": 0.85,
  "context_precision": 0.55,
  "context_recall": 0.52
}
```

**Interpr√©tation** :
- ‚úÖ Bonnes r√©ponses et fid√©lit√©
- ‚ö†Ô∏è Probl√®me de r√©cup√©ration de contexte
- **Cause probable** : Chunking sous-optimal, embeddings faibles
- **Action** : Revoir la strat√©gie d'indexation et de r√©cup√©ration

### Sc√©nario 3 : Probl√®me d'Hallucination

```json
{
  "faithfulness": 0.55,
  "answer_relevancy": 0.82,
  "context_precision": 0.75,
  "context_recall": 0.78
}
```

**Interpr√©tation** :
- ‚ö†Ô∏è Hallucinations fr√©quentes
- ‚úÖ Bon contexte r√©cup√©r√©
- **Cause probable** : Prompt trop permissif, temp√©rature trop √©lev√©e
- **Action** : Revoir le prompt, ajuster temp√©rature, forcer l'utilisation des sources

## üîÑ Processus d'Am√©lioration Continue

1. **Collecter** : Ex√©cuter √©valuations r√©guli√®res
2. **Analyser** : Identifier les patterns et tendances
3. **Diagnostiquer** : Trouver les causes des probl√®mes
4. **Am√©liorer** : Impl√©menter les corrections
5. **Valider** : Re-tester avec nouvelles √©valuations
6. **D√©ployer** : Mettre en production si valid√©
7. **Monitor** : Surveiller en continu

## üìö Ressources

- [RAGAS Documentation](https://docs.ragas.io/)
- [RAG Evaluation Best Practices](https://www.ragas.io/blog/evaluation)
- [Prompt Engineering Guide](https://www.promptingguide.ai/)

---

**Note** : Ces m√©triques et seuils sont des recommandations bas√©es sur les meilleures pratiques. Ils peuvent √™tre ajust√©s selon les besoins sp√©cifiques de votre projet.
